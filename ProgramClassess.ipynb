{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation():\n",
    "    def preprocessData(self,data):\n",
    "        import re\n",
    "        processed_tweets = []\n",
    "        for tweet in range(0, len(data)):\n",
    "            processed_tweet = data[tweet].encode().decode('unicode_escape') #remove unicode\n",
    "            processed_tweet = processed_tweet.encode('ascii', errors='ignore').decode(\"utf-8\")#remove unicode\n",
    "            processed_tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',processed_tweet)#remoce url\n",
    "            processed_tweet = re.sub('@[^ \\s]+','',processed_tweet)  #Remove username\n",
    "            processed_tweet = re.sub(r'\\W', ' ', processed_tweet) # Remove all the special characters\n",
    "            processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)# remove all single characters\n",
    "            processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet)  # Remove single characters from the start\n",
    "            processed_tweet = re.sub(r'[0-9]+', '', processed_tweet)#Remove number\n",
    "            processed_tweet = re.sub(r'^b\\s+', '', processed_tweet) # Removing prefixed 'b'\n",
    "            processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I) # Substituting multiple spaces with single space\n",
    "            processed_tweet = processed_tweet.lower()# Converting to Lowercase\n",
    "            processed_tweets.append(processed_tweet)\n",
    "        return processed_tweets\n",
    "    \n",
    "    def stemmingTweet(self,data):\n",
    "        from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "        stemmer = StemmerFactory().create_stemmer()\n",
    "        stemmed_data = []\n",
    "        for tweet in data:\n",
    "            stemmed_data.append(stemmer.stem(tweet)) # Stemming data\n",
    "        return stemmed_data\n",
    "    \n",
    "    def extractEmoji(self,data):\n",
    "        #from ipynb.fs.full.EmojidictCopy1 import emoji_dict\n",
    "        import pickle\n",
    "        import re\n",
    "        f = open('emoji_dict.pickle', 'rb')\n",
    "        emoji_dict = pickle.load(f)\n",
    "        f.close()\n",
    "        emoji,tweets_em = \"\", []\n",
    "        for i in range(len(data)):\n",
    "            processed_tweet = re.sub(r'\\\\', '/', data[i])\n",
    "            for j in emoji_dict.keys():\n",
    "                if(re.search(j,processed_tweet)):\n",
    "                    emoji = \" \" + emoji  + str(emoji_dict[j])\n",
    "                    j = re.sub(r'/', '\\\\\\\\', j)\n",
    "                    data[i]=data[i].replace(j, emoji)   \n",
    "            tweets_em.append(str(data[i]))\n",
    "            emoji = \"\"\n",
    "        return tweets_em\n",
    "    \n",
    "    def splitHashtag(self,data):\n",
    "        import re\n",
    "        t_hashtag = []\n",
    "        for x in data:\n",
    "            hashtag = re.findall(r\"#(\\w+)\", x)\n",
    "            for letter in hashtag:\n",
    "                splitted=''\n",
    "                for i in range(len(letter)):\n",
    "                    if len(splitted)==0: splitted+=\" \"+letter[i]       \n",
    "                    if(i+1 <len(letter)):\n",
    "                        if letter[i+1].islower():\n",
    "                            splitted += letter[i+1]\n",
    "                        elif letter[i].isupper() and letter[i+1].isupper():\n",
    "                            splitted += letter[i+1]\n",
    "                        else:\n",
    "                            splitted+=\" \"+letter[i+1]\n",
    "                x=x.replace(\"#\"+letter, splitted)\n",
    "            t_hashtag.append(x)\n",
    "        return t_hashtag     \n",
    "    \n",
    "    def removeStopword(self, data):\n",
    "        import nltk\n",
    "        stopword = ['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia', 'dua', 'ia', 'seperti','jika',\n",
    "                    'sehingga', 'kembali', 'dan', 'ini', 'karena', 'kepada', 'oleh', 'saat', 'harus', 'sementara', 'setelah',\n",
    "                    'kami', 'sekitar', 'bagi', 'serta', 'di', 'dari', 'telah', 'sebagai', 'masih', 'hal', 'ketika', 'adalah', 'itu',\n",
    "                    'dalam', 'bahwa', 'atau', 'hanya', 'kita', 'dengan', 'akan', 'juga', 'ada', 'mereka', 'saya', 'terhadap',\n",
    "                    'secara', 'agar', 'lain', 'anda', 'begitu', 'mengapa', 'kenapa', 'yaitu', 'yakni', 'daripada', 'itulah', 'lagi',\n",
    "                    'maka', 'tentang', 'demi', 'dimana', 'kemana', 'pula', 'sambil', 'sebelum', 'sesudah', 'supaya', 'guna', 'kah',\n",
    "                    'pun', 'sampai', 'sedangkan', 'selagi', 'sementara', 'tetapi', 'apakah', 'kecuali', 'sebab', 'selain', 'seolah', 'seraya', \n",
    "                    'seterusnya', 'agak', 'boleh', 'dapat', 'dsb', 'dst', 'dll', 'dahulu', 'dulunya', 'anu', 'demikian', 'tapi',\n",
    "                    'ingin', 'juga', 'mari', 'nanti', 'melainkan',  'ok', 'seharusnya', 'sebetulnya', 'setiap', 'setidaknya',\n",
    "                    'sesuatu', 'pasti', 'saja', 'toh', 'ya', 'walau', 'tolong', 'tentu', 'amat', 'apalagi', 'bagaimanapun',\n",
    "                    'yg','nya','nyang','nih','gue','di','ini','klo','kalau','n','pak','kok','dong',\n",
    "                    'nyuk','lah','dia','krn','utk','nyg','jadi','aja','apa','nkri','garuda','jakarta','papua','dki','jkt','bang',\n",
    "                   'jg','pa','aku','cc','bib','kalian','tagar','sy','el','bikin','baru','dg','cara','tp','dr','trus','sdh','gua','udh',\n",
    "                   'skrng','skrg','pd','jd','sm','ibu','bu','atas','bawah','dalam','dlm','tengah','dng','dg','awal',\n",
    "                   'Anies','Susi','anies','susi','ri','maruf','amin','mobil',\n",
    "                   'rektor','manusia','china','dgn','kemenkes','kementan','mentan','kira','apa','dp','bulan','agama','pem','rumah','mahfud',\n",
    "                   'org','anis','polisi','indonesia','jokowi','ari','askhara','baswedan','bapak','abah',\n",
    "                    'teh','minum','iris','tulang','belikat','buntal','hatta','leiliv','trent','unra','ten','asia','irfan','fauzy','size','m',\n",
    "                    'xl','xxl','pcs','note','lis','dahlia','gula','fafa','on','the','floor'\n",
    "                    'gubernur','menteri','anak','kampus','kota','bpjs','beliau','ku','rakyat','guys','mahasiswa','wakil','sekolah','dprd','negri',\n",
    "                    'apbd','waktunya','saatnya','jawa','and','with','floor','of','and','hari','sekarang','gubernur']\n",
    "        preprocessed_tweets = []\n",
    "        text = \"\"\n",
    "        for t in data:\n",
    "            tokens = nltk.word_tokenize(t)\n",
    "            for token in tokens:\n",
    "                if(token not in stopword):\n",
    "                    text+=\" \"+token\n",
    "            preprocessed_tweets.append(text)\n",
    "            text = \"\"\n",
    "        return preprocessed_tweets\n",
    "    def extractInterjection(self,data):\n",
    "        import nltk\n",
    "        interjeksi =[\"bah\", \"ih, idih\",\"cih\",\"cis\",\"brengsek\",\"sialan\",\"keparat\",\"celaka\",\"buset\",\"tolol\",\n",
    "              \"asik\",\"asyik\",\"aduhai\",\"amboi\",\"syukur\",\"untung\",\"alhamdulillah\",\"yay\",\"semoga\",\n",
    "              \"mudah-mudahan\",\"moga-moga\",\"aduh\",\"eh\",\"oh\",\"aih\",\"lho\",\"lo\",\"loh\",\"astaga\",\"alamak\",\"gila\",\n",
    "              \"gile\",\"wow\",\"waw\",\"wah\",\"wew\",\"amazing\",\"congrats\",\"wuar biasa\",\"luar biasa\",\"nah\",\"mari\",\"ayo\"]\n",
    "        interjeksi_column = []\n",
    "        k = 0\n",
    "        for t in data:\n",
    "            tokens = nltk.word_tokenize(t)\n",
    "            for token in tokens:\n",
    "                if(token in interjeksi):\n",
    "                    k+=1\n",
    "            interjeksi_column.append(k)\n",
    "            k = 0\n",
    "        return interjeksi_column\n",
    "    \n",
    "    def translateTweet(self, data):\n",
    "        from googletrans import Translator\n",
    "        translator = Translator()\n",
    "        translate = []\n",
    "        for x in data:\n",
    "            t = translator.translate(x, src='id', dest='en').text\n",
    "            translate.append(t)\n",
    "        return translate\n",
    "    \n",
    "    def splitTweet(self,data):\n",
    "        import nltk\n",
    "        awal = \"\"\n",
    "        akhir = \"\"\n",
    "        t_awal = []\n",
    "        t_akhir = []\n",
    "        for ptweet in data:\n",
    "            tokens = nltk.word_tokenize(ptweet)\n",
    "            n = len(tokens)\n",
    "            awal  = ' '.join(tokens[0:int(n/2)])\n",
    "            akhir = ' '.join(tokens[int(n/2):n])\n",
    "            t_awal.append(awal)\n",
    "            t_akhir.append(akhir)\n",
    "            awal=\"\"\n",
    "            akhir=\"\"\n",
    "        return t_awal, t_akhir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentScore():\n",
    "    \n",
    "    def split_line(self, line):\n",
    "        cols = line.split(\"\\t\")\n",
    "        return cols\n",
    "\n",
    "    def get_words(self,cols):\n",
    "        words_ids = cols[4].split(\" \")\n",
    "        words = [w.split(\"#\")[0] for w in words_ids]\n",
    "        return words\n",
    "\n",
    "    def get_positive(self,cols):\n",
    "        col = float(cols[2])\n",
    "        if (col>=0.5):\n",
    "            col = col*3\n",
    "        return col\n",
    "\n",
    "    def get_negative(self,cols):\n",
    "        col = float(cols[3])\n",
    "        if(col>=0.5):\n",
    "            col = col*3\n",
    "        return col\n",
    "\n",
    "    def get_scores(self,filepath, text):\n",
    "        f = open(filepath)\n",
    "        count =0.0\n",
    "        totalpositive =0.0\n",
    "        totalnegative =0.0\n",
    "        for line in f:\n",
    "            if not line.startswith(\"#\"):\n",
    "                cols = self.split_line(line)\n",
    "                words = self.get_words(cols)\n",
    "                for i in range(len(text)):\n",
    "                    if text[i] in words:\n",
    "                        if text[i-1] == \"not\":\n",
    "                            totalnegative = totalnegative +float(self.get_positive(cols))\n",
    "                            totalpositive = totalpositive + float(self.get_negative(cols))\n",
    "                        else:\n",
    "                            totalpositive = totalpositive + float(self.get_positive(cols))\n",
    "                            totalnegative = totalnegative + float(self.get_negative(cols))\n",
    "                        count =count + 1\n",
    "        #sentimentscore = ((p*Pw + pw)-(p*Nw+nw))/((p.Pw+pw)+(p.Nw+nw))\n",
    "        if(totalpositive ==0 and totalnegative == 0):\n",
    "            sentimentscore = 0\n",
    "        else:\n",
    "            sentimentscore = (totalpositive-totalnegative)/(totalpositive+totalnegative)\n",
    "\n",
    "        return sentimentscore\n",
    "    \n",
    "    def main(self, data):\n",
    "        from nltk.corpus import stopwords\n",
    "        scores = []\n",
    "        for tweet in data:\n",
    "            text = str(tweet).split(\" \")\n",
    "            filtered_sentence = [w for w in text]\n",
    "            scores.append(self.get_scores(\"SentiWordNet_3.0.0.txt\",filtered_sentence))\n",
    "            \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFValue():\n",
    "\n",
    "    def word_freq(self, processed_tweets,freq):\n",
    "        import nltk\n",
    "        wordfreq = {}\n",
    "        for tweet in processed_tweets:\n",
    "            for token in nltk.word_tokenize(tweet):\n",
    "                 wordfreq[token] = 1 if token not in wordfreq.keys() else wordfreq[token]+1\n",
    "        most_freq = heapq.nlargest(freq, wordfreq, key=wordfreq.get)\n",
    "        return most_freq\n",
    "\n",
    "    def idf_value(self,most_freq, processed_tweets):\n",
    "        import nltk\n",
    "        import numpy as np\n",
    "        idf_values = {}\n",
    "        for token in most_freq:\n",
    "            token_tweet = 0\n",
    "            for tweet in processed_tweets:\n",
    "                if token in nltk.word_tokenize(tweet):\n",
    "                    token_tweet +=1\n",
    "            idf_values[token] = 0 if token_tweet==0 else np.log(len(processed_tweets)/(token_tweet))\n",
    "        return idf_values\n",
    "\n",
    "    def tf_value(self,most_freq, processed_tweets):\n",
    "        import nltk\n",
    "        word_tf_values = {}\n",
    "        for token in most_freq:\n",
    "            tf = []\n",
    "            for tweet in processed_tweets:\n",
    "                term_freq = 0\n",
    "                for word in nltk.word_tokenize(tweet):\n",
    "                    if token == word: term_freq +=1\n",
    "                word_tf = term_freq/len(nltk.word_tokenize(tweet))\n",
    "                tf.append(word_tf)\n",
    "            word_tf_values[token] =tf\n",
    "        return word_tf_values\n",
    "\n",
    "    def tf_idf_value(self,word_tf_values, idf_values):  \n",
    "        tfidf_x = []\n",
    "        for token in word_tf_values.keys():\n",
    "            tfidf_tweet = []  \n",
    "            for tf_tweet in word_tf_values[token]:\n",
    "                tfidf_tweet.append(tf_tweet * idf_values[token])\n",
    "            tfidf_x.append(tfidf_tweet)\n",
    "        return tfidf_x\n",
    "    def main(self,mostfreqword,processed_tweets):\n",
    "        import heapq\n",
    "        import numpy as np\n",
    "        #mostfreqword = self.word_freq(processed_tweets)\n",
    "        tweetidfvalue = self.idf_value(mostfreqword,processed_tweets)\n",
    "        tweettfvalue = self.tf_value(mostfreqword, processed_tweets)\n",
    "        tweettfidfvalue = self.tf_idf_value(tweettfvalue, tweetidfvalue)\n",
    "        Xn = np.asarray(tweettfidfvalue)\n",
    "        Xn = np.transpose(Xn)\n",
    "        return Xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART():\n",
    "    #split data dr nilai kolom\n",
    "    def test_split(self,index, value, dataset):\n",
    "        left, right = list(), list()\n",
    "        for row in dataset: left.append(row) if row[index]<value else right.append(row)\n",
    "        return left, right\n",
    "    \n",
    "    def gini_index(self, groups, classes): \n",
    "        n_instances = float(sum([len(group) for group in groups]))\n",
    "        gini = 0.0\n",
    "        for group in groups:\n",
    "            size = float(len(group))\n",
    "            if size == 0:\n",
    "                continue\n",
    "            score = 0.0\n",
    "            # score the group based on the score for each class\n",
    "            for class_val in classes:\n",
    "                p = [row[-1] for row in group].count(class_val) / size\n",
    "                score += p**2\n",
    "            # weight the group score by its relative size\n",
    "            gini += (1.0 - score) * (size / n_instances)\n",
    "        return gini\n",
    "    \n",
    "    def get_best_split(self,dataset):\n",
    "        class_values = list(set(row[-1] for row in dataset))\n",
    "        b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "        for index in range(len(dataset[0])-1):\n",
    "            for row in dataset:\n",
    "                groups = self.test_split(index, row[index], dataset)\n",
    "                gini = self.gini_index(groups, class_values)\n",
    "                if gini < b_score:\n",
    "                    b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "        return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "    def to_terminal(self,group):\n",
    "        outcomes = [row[-1] for row in group]\n",
    "        return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "    def split(self,node):\n",
    "        #print(\"Def Split\")\n",
    "        left, right = node['groups']\n",
    "        del(node['groups'])\n",
    "        # check for a no split\n",
    "        if not left or not right:\n",
    "            node['left'] = node['right'] = self.to_terminal(left + right)\n",
    "            return\n",
    "        # process left child\n",
    "        if len(left) <= 1:\n",
    "            node['left'] = self.to_terminal(left)\n",
    "        else:\n",
    "            node['left'] = self.get_best_split(left)\n",
    "            self.split(node['left'])\n",
    "        # process right child\n",
    "        if len(right) <= 1:\n",
    "            node['right'] = self.to_terminal(right)\n",
    "        else:\n",
    "            node['right'] = self.get_best_split(right)\n",
    "            self.split(node['right'])\n",
    "    def build_tree(self,train):\n",
    "        root = self.get_best_split(train)\n",
    "        self.split(root)\n",
    "        return root\n",
    "    def predict(self, node, row):\n",
    "        if row[node['index']] < node['value']:\n",
    "            return self.predict(node['left'], row) if isinstance(node['left'], dict) else  node['left']\n",
    "        else:\n",
    "            return self.predict(node['right'], row) if isinstance(node['right'], dict) else node['right']\n",
    "    def decision_tree(self,tree, test):\n",
    "        predictions = list()\n",
    "        for row in test:  predictions.append(self.predict(tree, row))   \n",
    "        return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    import pandas as pd\n",
    "    def bootstrapping(self,train_df, n_bootstrap,seed):\n",
    "        import random\n",
    "        random.seed(seed)\n",
    "        bootstrap_index = [random.randint(0, len(train_df)-1) for i in range(0,n_bootstrap)]\n",
    "        return train_df.iloc[bootstrap_index]  \n",
    "    def random_forest_algorithm(self,train_df, n_trees, n_bootstrap):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        forest = []\n",
    "        c = CART()\n",
    "        random.seed(0)\n",
    "        r_seed = [random.randint(0, len(train_df)-1) for i in range(0,n_trees)]\n",
    "        #r_seed = random.sample(range(0, len(train_df)-1), n_trees)\n",
    "        for i in range(n_trees):\n",
    "            df_bootstrapped = self.bootstrapping(train_df, n_bootstrap, r_seed[i])\n",
    "            forest.append(c.build_tree(np.array(df_bootstrapped)))\n",
    "            print(i)\n",
    "        return forest\n",
    "    def random_forest_predictions(self,forest,test_df):\n",
    "        import pandas as pd\n",
    "        df_predictions = {}\n",
    "        c = CART()\n",
    "        for i in range(len(forest)):\n",
    "            column_name = \"tree_{}\".format(i)\n",
    "            df_predictions[column_name] = c.decision_tree(tree=forest[i],test=test_df)\n",
    "        random_forest_predictions = pd.DataFrame(df_predictions).mode(axis=1)[0]\n",
    "        return random_forest_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    from math import sqrt\n",
    "    from math import exp\n",
    "    from math import pi\n",
    "    #Mean\n",
    "    def mean(self,numbers):\n",
    "        return sum(numbers)/float(len(numbers))\n",
    "    def variance(self,numbers,_epsilon):\n",
    "        from math import sqrt\n",
    "        avg = self.mean(numbers)\n",
    "        v = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "        return sqrt(v) + _epsilon \n",
    "    def divide_by_class(self,dataset, label):\n",
    "        division = dict()\n",
    "        for i in range(len(dataset)):\n",
    "            vector = dataset[i]\n",
    "            class_label = label[i]\n",
    "            if (class_label not in division):\n",
    "                division[class_label] = list()\n",
    "            division[class_label].append(vector)\n",
    "        return division\n",
    "    def dataset_values(self,dataset,_epsilon):\n",
    "        values = [(self.mean(column), self.variance(column,_epsilon), len(column)) for column in zip(*dataset)]\n",
    "        return values\n",
    "    #Mean, variance tiap kelas\n",
    "    def values_by_class(self,dataset, label):\n",
    "        division = self.divide_by_class(dataset, label)\n",
    "        values = dict()\n",
    "        for class_label, rows in division.items():\n",
    "            values[class_label] = self.dataset_values(rows,0)\n",
    "        l=[]\n",
    "        for x in list(values.values()):\n",
    "            for y in x:\n",
    "                l.append(y[1])\n",
    "        var_smoothing = 1e-3\n",
    "        _epsilon = var_smoothing * max(l)\n",
    "        values = dict()\n",
    "        for class_label, rows in division.items():\n",
    "            values[class_label] = self.dataset_values(rows,_epsilon)\n",
    "        return values\n",
    "    def calculate_probability(self,x, mean, variance):\n",
    "        from math import sqrt\n",
    "        from math import exp\n",
    "        from math import pi\n",
    "        exponent = exp(-((x-mean)**2 / (2 * variance**2 )))\n",
    "        return (1 / (sqrt(2 * pi) * variance)) * exponent\n",
    "    def calculate_class_probabilities(self,values, row):\n",
    "        total_rows = sum([values[label][0][2] for label in values])\n",
    "        probabilities = dict()\n",
    "        for class_label, class_values in values.items():\n",
    "            probabilities[class_label] = values[class_label][0][2]/float(total_rows)\n",
    "            for i in range(len(class_values)):\n",
    "                mean, stdev, _ = class_values[i]\n",
    "                probabilities[class_label] *= self.calculate_probability(row[i], mean, stdev)\n",
    "        return probabilities\n",
    "    def predict_nb(self,values, row):\n",
    "        probabilities = self.calculate_class_probabilities(values, row)\n",
    "        best_label, best_prob = None, -1\n",
    "        for class_label, probability in probabilities.items():\n",
    "            if best_label is None or probability > best_prob:\n",
    "                best_prob = probability\n",
    "                best_label = class_label\n",
    "        return best_label\n",
    "    def naive_bayes(self,values,x_test):\n",
    "        predictions = list()\n",
    "        for row in x_test:\n",
    "            output = self.predict_nb(values, row)\n",
    "            predictions.append(output)\n",
    "        return(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
